{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "04ac910d",
      "metadata": {
        "id": "04ac910d"
      },
      "source": [
        "# Cannabis Grow Segmentation with TorchGeo (torchgeo-compatible version)\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wxmiked/cannabis-deforestation/blob/main/notebooks/cannabis-segmentation-torchgeo.ipynb)\n",
        "\n",
        "This script refactors the dataset and dataloaders to use torchgeo's GeoDataset and RandomGeoSampler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nuyXOeXFjK8c",
      "metadata": {
        "id": "nuyXOeXFjK8c"
      },
      "outputs": [],
      "source": [
        "pip install torchgeo segmentation-models-pytorch >& /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n7f7mQ7kkybV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7f7mQ7kkybV",
        "outputId": "6e6c80d2-784a-46be-8c71-f6ec15113bce"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # These commands will only run in Google Colab\n",
        "    !git clone  --single-branch https://github.com/wxmiked/cannabis-deforestation.git # -b feature-use-parcel-bounding-box-instead-of-parcel-shape\n",
        "    !mv cannabis-deforestation/* .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41881006",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41881006",
        "lines_to_next_cell": 1,
        "outputId": "06446a7b-3b2c-48df-eb1c-0523d9a98551"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchgeo.datasets import GeoDataset\n",
        "from torchgeo.samplers import RandomGeoSampler\n",
        "from torchgeo.transforms import AugmentationSequential\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from shapely.geometry import shape, box\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from shapely.geometry import Polygon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e641ecf",
      "metadata": {
        "id": "5e641ecf"
      },
      "outputs": [],
      "source": [
        "# ────────── Cell: Fix CannabisSegmentationGeoDataset ──────────\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "from shapely.geometry import Polygon\n",
        "from torchgeo.datasets import RasterDataset\n",
        "from rtree import index as rindex\n",
        "import re\n",
        "from datetime import datetime\n",
        "from rasterio.windows import Window\n",
        "from rasterio.features import rasterize\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "class CannabisSegmentationGeoDataset(RasterDataset):\n",
        "    def __init__(self, annotations_dir, naip_root, transforms=None):\n",
        "        self.annotation_paths = sorted(Path(annotations_dir).glob(\"*.json\"))\n",
        "        self.naip_root = Path(naip_root)\n",
        "        self.filename_glob = \"*_*.tif\"\n",
        "        self.filename_regex = r\"^.*_(?P<date>\\d{8})\"\n",
        "        self.date_format = \"%Y%m%d\"\n",
        "        self.is_image = True\n",
        "        self.separate_files = False\n",
        "        self.all_bands = (\"R\", \"G\", \"B\", \"NIR\")\n",
        "        self.rgb_bands = (\"R\", \"G\", \"B\",)\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # Build a mapping from ann_path → image_path\n",
        "        self.image_paths = {}\n",
        "        for ann_path in self.annotation_paths:\n",
        "            ann = json.load(open(ann_path))\n",
        "            self.image_paths[ann_path] = self.naip_root / ann[\"imagePath\"]\n",
        "\n",
        "        super().__init__(paths=self.image_paths.values(), transforms=transforms)\n",
        "\n",
        "        # 1) compute new_bounds\n",
        "        self._bounds = []\n",
        "        for ann_path, img_path in self.image_paths.items():\n",
        "            with rasterio.open(img_path) as src:\n",
        "                self._crs = src.crs\n",
        "                b = src.bounds\n",
        "                self._bounds.append((b.left, b.bottom, b.right, b.top))\n",
        "\n",
        "        # 3) set the dates\n",
        "        dates = []\n",
        "        for fname in self.image_paths.values():\n",
        "            match = re.match(r\"^.*_(\\d{8})\", fname.name)\n",
        "            if match:\n",
        "                date = datetime.strptime(match.group(1), \"%Y%m%d\")\n",
        "                dates.append(date.timestamp())\n",
        "        if dates:\n",
        "            self.mint = min(dates)\n",
        "            self.maxt = max(dates)\n",
        "        else:\n",
        "            self.mint = 0\n",
        "            self.maxt = 9223372036854775807\n",
        "\n",
        "        # 4) update bounds with mint and maxt\n",
        "        self._bounds = [(b[0], b[1], b[2], b[3], self.mint, self.maxt) for b in self._bounds]\n",
        "\n",
        "        # 2) rebuild index by hand\n",
        "        p = rindex.Property()\n",
        "        p.dimension = 3\n",
        "        self.index = rindex.Index(properties=p)\n",
        "        for i, b in enumerate(self._bounds):\n",
        "            self.index.insert(i, b)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotation_paths)\n",
        "\n",
        "    def __getitem__(self, query):\n",
        "        # Extract size from BoundingBox (or fallback to self.size)\n",
        "        size = getattr(query, \"size\", self.size if hasattr(self, 'size') else 128)\n",
        "        bbox = query[\"bbox\"]\n",
        "        ann_idx = query[\"ann_idx\"]\n",
        "\n",
        "        if size is None:\n",
        "            raise ValueError(\"Query dict must include 'size'\")\n",
        "\n",
        "        # # 2) 3-D vs 2-D R-tree lookup\n",
        "        # bbox = tuple(float(x) for x in bbox)\n",
        "        # if self.index.properties.dimension == 3:\n",
        "        #     lookup = (bbox[0], bbox[1], self.mint, bbox[2], bbox[3], self.maxt)\n",
        "        # else:\n",
        "        #     lookup = bbox\n",
        "        # idxs = list(self.index.intersection(lookup))\n",
        "        # if not idxs:\n",
        "        #     raise ValueError(f\"No annotation intersects bbox {bbox}\")\n",
        "        ann_path = self.annotation_paths[ann_idx]\n",
        "        img_path = self.image_paths[ann_path]\n",
        "\n",
        "        # 3) open & read fixed window\n",
        "        with rasterio.open(img_path) as src:\n",
        "            # center in world → pixel\n",
        "            cx, cy = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n",
        "            rowc, colc = src.index(cx, cy)\n",
        "\n",
        "            half = size // 2\n",
        "            col_off = colc - half\n",
        "            row_off = rowc - half\n",
        "\n",
        "            # Check center in bounds\n",
        "            if not (0 <= colc < src.width and 0 <= rowc < src.height):\n",
        "                # print(f\"[DEBUG] OOB center → bbox={bbox}, center_px=({colc},{rowc}), size={size}\")\n",
        "                return None\n",
        "\n",
        "            # Check window fits\n",
        "            if col_off < 0 or row_off < 0 or col_off+size > src.width or row_off+size > src.height:\n",
        "                # print(f\"[DEBUG] Window out of image → bbox={bbox}, window=({col_off},{row_off},{size},{size})\")\n",
        "                return None\n",
        "\n",
        "            window = Window(col_off, row_off, size, size)\n",
        "\n",
        "            # Read image with this window\n",
        "            image = src.read(window=window)\n",
        "\n",
        "            # After reading image\n",
        "            if image.shape[1] != size or image.shape[2] != size:\n",
        "                # print(f\"[DEBUG] Wrong patch size → image.shape={image.shape}, wanted {size}\")\n",
        "                return None\n",
        "\n",
        "            _, actual_height, actual_width = image.shape\n",
        "            if np.all(image == 0):\n",
        "                # print(f\"Warning: Completely blank image patch at {window}\")\n",
        "                return None\n",
        "\n",
        "            # build mask geometries\n",
        "            ann = json.load(open(ann_path))\n",
        "            shapes = [\n",
        "                (labelme_points_to_geopolygon(s[\"points\"], src.transform), 1)\n",
        "                for s in ann[\"shapes\"] if s[\"label\"] != \"cannabis - old\"\n",
        "            ]\n",
        "\n",
        "            # Get the transform for the ACTUAL window\n",
        "            actual_transform = src.window_transform(window)\n",
        "\n",
        "            # print(f\"col_off: {col_off}, row_off: {row_off}, window: {window}\")\n",
        "            # print(f\"actual_transform: {actual_transform}\")\n",
        "\n",
        "            # Rasterize mask with the actual transform and shape\n",
        "            mask = rasterize(\n",
        "                        shapes,\n",
        "                        out_shape=(actual_height, actual_width),\n",
        "                        transform=actual_transform,\n",
        "                        fill=0,\n",
        "                        dtype=\"uint8\",\n",
        "                    )[None]\n",
        "\n",
        "            assert image.shape[1:] == mask.shape[1:], f\"Image and mask shapes do not match: {image.shape}, {mask.shape}\"\n",
        "\n",
        "        # 4) Transformations\n",
        "        if self.transforms:\n",
        "            # Apply transforms without normalization\n",
        "            image, mask_transformed = self.transforms(image, mask[0])\n",
        "            mask = mask_transformed\n",
        "\n",
        "        # Make sure image is a tensor before normalization\n",
        "        if not isinstance(image, torch.Tensor):\n",
        "            image = torch.from_numpy(image).float()\n",
        "        else:\n",
        "            image = image.float()\n",
        "\n",
        "        # Manual normalization with proper tensor types\n",
        "        num_channels = image.shape[0]\n",
        "        if num_channels == 4:\n",
        "            mean = torch.tensor([0.485, 0.456, 0.406, 0.5], device=image.device)\n",
        "            std = torch.tensor([0.229, 0.224, 0.225, 0.2], device=image.device)\n",
        "        elif num_channels == 5:  # After NDVI is added\n",
        "            mean = torch.tensor([0.485, 0.456, 0.406, 0.5, 0.0], device=image.device)\n",
        "            std = torch.tensor([0.229, 0.224, 0.225, 0.2, 1.0], device=image.device)\n",
        "        else:\n",
        "            # Default fallback\n",
        "            mean = torch.ones(num_channels, device=image.device) * 0.5\n",
        "            std = torch.ones(num_channels, device=image.device) * 0.2\n",
        "\n",
        "        # Apply normalization (now both are tensors)\n",
        "        image = (image - mean[:, None, None]) / std[:, None, None]\n",
        "\n",
        "        # Return in expected format\n",
        "        return {\"image\": image, \"mask\": mask, \"filename\": img_path.name.split(\"/\")[-1]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45884673",
      "metadata": {
        "id": "45884673"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b48ebbe",
      "metadata": {
        "id": "9b48ebbe"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "random.seed() # default is system time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65cd337a",
      "metadata": {
        "id": "65cd337a"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "import json, random, rasterio\n",
        "from rasterio.transform import xy\n",
        "from shapely.geometry import Polygon, box, Point\n",
        "from torchgeo.samplers import RandomGeoSampler\n",
        "\n",
        "def is_valid_bbox(bbox, left, bottom, right, top, polys=None, require_no_intersection=False):\n",
        "    \"\"\"Check if bbox is within bounds and (optionally) does not intersect any polygons.\"\"\"\n",
        "    within_bounds = (\n",
        "        bbox[0] >= left and bbox[2] <= right and\n",
        "        bbox[1] >= bottom and bbox[3] <= top\n",
        "    )\n",
        "    if not within_bounds:\n",
        "        return False\n",
        "    if require_no_intersection and polys is not None:\n",
        "        bbox_poly = box(*bbox)\n",
        "        return not any(p.intersects(bbox_poly) for p in polys)\n",
        "    return True\n",
        "\n",
        "def random_point_in_polygon(polygon):\n",
        "    minx, miny, maxx, maxy = polygon.bounds\n",
        "    for _ in range(50):  # Try up to 50 times\n",
        "        p = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
        "        if polygon.contains(p):\n",
        "            return p.x, p.y\n",
        "    return None, None  # Failed\n",
        "\n",
        "def labelme_points_to_geopolygon(points, transform):\n",
        "    return Polygon([transform * (x, y) for x, y in points])\n",
        "\n",
        "class PositiveNegativeGeoSampler(RandomGeoSampler):\n",
        "    def __init__(self, dataset, size, length, pos_fraction=0.5, num_pos_per_poly=1, num_neg_per_image=2, max_attempts=50):\n",
        "        super().__init__(dataset, size, length)\n",
        "        self.ds = dataset\n",
        "        self.size = size\n",
        "        self.length = length\n",
        "        self.pos_frac = pos_fraction\n",
        "        self.pos_bboxes: List[Tuple[float, float, float, float]] = []\n",
        "        self.neg_bboxes: List[Tuple[float, float, float, float]] = []\n",
        "\n",
        "        for i, ann_path in enumerate(self.ds.annotation_paths):\n",
        "            ann = json.load(open(ann_path))\n",
        "            img_path = self.ds.image_paths[ann_path]\n",
        "            with rasterio.open(img_path) as src:\n",
        "                w, h = src.width, src.height\n",
        "                if w < size or h < size:\n",
        "                    print(f\"Skipping {img_path} (size {w}x{h} < patch {size})\")\n",
        "                    continue\n",
        "\n",
        "                left, bottom, right, top = src.bounds\n",
        "                rx, ry = src.res\n",
        "                half_w = (size / 2) * rx\n",
        "                half_h = (size / 2) * abs(ry)\n",
        "                half_w_px = size // 2\n",
        "                half_h_px = size // 2\n",
        "                polys = []\n",
        "\n",
        "                # Collect all polygons\n",
        "                for shape_ in ann[\"shapes\"]:\n",
        "                    if shape_[\"label\"] == \"cannabis - old\":\n",
        "                        continue\n",
        "                    poly = labelme_points_to_geopolygon(shape_[\"points\"], src.transform)\n",
        "                    polys.append(poly)\n",
        "\n",
        "                # Positive sampling: random points in each polygon\n",
        "                for poly in polys:\n",
        "                    for _ in range(num_pos_per_poly):\n",
        "                        for attempt in range(max_attempts):\n",
        "                            cx, cy = random_point_in_polygon(poly)\n",
        "                            if cx is None:\n",
        "                                continue\n",
        "                            pb = (cx - half_w, cy - half_h, cx + half_w, cy + half_h)\n",
        "                            if is_valid_bbox(pb, left, bottom, right, top):\n",
        "                                self.pos_bboxes.append((i, pb))\n",
        "                                break\n",
        "\n",
        "                # Debug\n",
        "                print(f\"{ann_path}: found {len(polys)} positive polygons, {len(self.pos_bboxes)} positive bboxes so far\")\n",
        "\n",
        "                # Negative sampling: random points not intersecting any polygon\n",
        "                if (w - half_w_px - 1) >= half_w_px and (h - half_h_px - 1) >= half_h_px:\n",
        "                    for _ in range(num_neg_per_image * max(1, len(polys))):\n",
        "                        for attempt in range(max_attempts):\n",
        "                            ix = random.randint(half_w_px, w - half_w_px - 1)\n",
        "                            iy = random.randint(half_h_px, h - half_h_px - 1)\n",
        "                            cx2, cy2 = xy(src.transform, iy, ix)\n",
        "                            nb = (\n",
        "                                cx2 - half_w,\n",
        "                                cy2 - half_h,\n",
        "                                cx2 + half_w,\n",
        "                                cy2 + half_h,\n",
        "                            )\n",
        "                            if is_valid_bbox(nb, left, bottom, right, top, polys, require_no_intersection=True):\n",
        "                                self.neg_bboxes.append((i, nb))\n",
        "                                break\n",
        "                else:\n",
        "                    print(f\"Skipping negative sampling for {img_path} (not enough room for patch)\")\n",
        "\n",
        "        print(f\"Positive patches: {len(self.pos_bboxes)}, Negative patches: {len(self.neg_bboxes)}\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        random.seed()\n",
        "        pos_count = int(self.length * self.pos_frac)\n",
        "        neg_count = self.length - pos_count\n",
        "\n",
        "        already_yielded = set()\n",
        "        available_pos = list(self.pos_bboxes)\n",
        "        available_neg = list(self.neg_bboxes)\n",
        "        random.shuffle(available_pos)\n",
        "        random.shuffle(available_neg)\n",
        "\n",
        "        # Yield positive samples\n",
        "        pos_yielded = 0\n",
        "        for ann_idx, bbox in available_pos:\n",
        "            if bbox in already_yielded:\n",
        "                continue\n",
        "            already_yielded.add(bbox)\n",
        "            pos_yielded += 1\n",
        "            yield {\"ann_idx\": ann_idx, \"bbox\": bbox, \"size\": self.size}\n",
        "            if pos_yielded >= pos_count:\n",
        "                break\n",
        "\n",
        "        # Yield negative samples\n",
        "        neg_yielded = 0\n",
        "        for ann_idx, bbox in available_neg:\n",
        "            if bbox in already_yielded:\n",
        "                continue\n",
        "            already_yielded.add(bbox)\n",
        "            neg_yielded += 1\n",
        "            yield {\"ann_idx\": ann_idx, \"bbox\": bbox, \"size\": self.size}\n",
        "            if neg_yielded >= neg_count:\n",
        "                break\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4039c148",
      "metadata": {
        "id": "4039c148"
      },
      "outputs": [],
      "source": [
        "def collate_skip_none(batch):\n",
        "    # Skip any samples that are None\n",
        "    count_none = sum(1 for x in batch if x is None)\n",
        "    # print(f\"count of  None in batch: {count_none}, total count: {len(batch)}\")\n",
        "    batch = [x for x in batch if x is not None]\n",
        "    # If batch is empty, you may want to handle it (raise, return dummy, etc.)\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "    # Let default_collate handle the rest\n",
        "    from torch.utils.data._utils.collate import default_collate\n",
        "    return default_collate(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bceecff2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bceecff2",
        "outputId": "d1eeabd2-9286-4e44-c5d2-31fa3e469a8d"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "!ls -l ../cannabis-parcels/cannabis-parcels-masked\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "naip_root = \"../cannabis-parcels/cannabis-parcels-masked\"\n",
        "annotations_dir = \"../cannabis-parcels/cannabis-parcels-masked\"\n",
        "dataset = CannabisSegmentationGeoDataset(\n",
        "    naip_root=naip_root,\n",
        "    annotations_dir=annotations_dir\n",
        ")\n",
        "\n",
        "# ─── Usage ───\n",
        "patch_size = 256\n",
        "sampler = PositiveNegativeGeoSampler(\n",
        "    dataset, size=patch_size, length=1000, pos_fraction=0.9, num_pos_per_poly=10, num_neg_per_image=5\n",
        ")\n",
        "loader = DataLoader(dataset, batch_size=32, sampler=sampler, collate_fn=collate_skip_none)\n",
        "\n",
        "# Visualize one batch\n",
        "batch = next(iter(loader))\n",
        "imgs, msks, fnames = batch[\"image\"], batch[\"mask\"], batch[\"filename\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd92eb82",
      "metadata": {
        "id": "dd92eb82"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a79ccd24",
      "metadata": {
        "id": "a79ccd24",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchgeo.transforms import AppendNDVI\n",
        "\n",
        "def get_transforms(patch_size):\n",
        "    class MinimalTransform:\n",
        "        def __init__(self, is_train=True):\n",
        "            self.is_train = is_train\n",
        "            # No normalization here - we'll add it separately\n",
        "            self.ndvi = AppendNDVI(index_red=0, index_nir=3)\n",
        "\n",
        "        def __call__(self, image, mask=None):\n",
        "            # Convert to tensor if needed\n",
        "            if not isinstance(image, torch.Tensor):\n",
        "                image = torch.from_numpy(image).float()\n",
        "            else:\n",
        "                image = image.float()\n",
        "\n",
        "            if mask is not None and not isinstance(mask, torch.Tensor):\n",
        "                mask = torch.from_numpy(mask).float()\n",
        "                if mask.dim() == 2:\n",
        "                    mask = mask.unsqueeze(0)\n",
        "\n",
        "            # Simple augmentations\n",
        "            if self.is_train:\n",
        "                # Horizontal flip\n",
        "                if torch.rand(1).item() < 0.5:\n",
        "                    image = torch.flip(image, dims=[2])\n",
        "                    if mask is not None:\n",
        "                        mask = torch.flip(mask, dims=[2])\n",
        "\n",
        "                # Vertical flip\n",
        "                if torch.rand(1).item() < 0.5:\n",
        "                    image = torch.flip(image, dims=[1])\n",
        "                    if mask is not None:\n",
        "                        mask = torch.flip(mask, dims=[1])\n",
        "\n",
        "                # Rotation\n",
        "                if torch.rand(1).item() < 0.5:\n",
        "                    k = torch.randint(1, 4, (1,)).item()\n",
        "                    image = torch.rot90(image, k=k, dims=[1, 2])\n",
        "                    if mask is not None:\n",
        "                        mask = torch.rot90(mask, k=k, dims=[1, 2])\n",
        "\n",
        "            # Skip normalization completely\n",
        "\n",
        "            # Apply NDVI transform\n",
        "            image = self.ndvi(image)\n",
        "\n",
        "            return image, mask\n",
        "\n",
        "    train_transform = MinimalTransform(is_train=True)\n",
        "    val_transform = MinimalTransform(is_train=False)\n",
        "    return train_transform, val_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bbf5c3f",
      "metadata": {
        "id": "9bbf5c3f"
      },
      "outputs": [],
      "source": [
        "## U-Net Model Architecture (unchanged)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, skip_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(out_channels + skip_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        if x.shape[-2:] != skip.shape[-2:]:\n",
        "            x = F.interpolate(x, size=skip.shape[-2:], mode='bilinear', align_corners=True)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class UNetWithResNet(nn.Module):\n",
        "    def __init__(self, n_channels=4, n_classes=1, pretrained=True):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet50(weights='IMAGENET1K_V2' if pretrained else None)\n",
        "        # Modify first conv layer to accept 4 channels\n",
        "        w = resnet.conv1.weight\n",
        "        resnet.conv1 = nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        if n_channels == 4:\n",
        "            resnet.conv1.weight.data[:, :3, :, :] = w.data\n",
        "            resnet.conv1.weight.data[:, 3:, :, :].zero_()\n",
        "        self.firstconv = resnet.conv1\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "        self.firstmaxpool = resnet.maxpool\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "        self.encoder4 = resnet.layer4\n",
        "        self.center = nn.Sequential(\n",
        "            nn.Conv2d(2048, 2048, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(2048),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.decoder4 = DecoderBlock(2048, 1024, 1024)\n",
        "        self.decoder3 = DecoderBlock(1024, 512, 512)\n",
        "        self.decoder2 = DecoderBlock(512, 256, 256)\n",
        "        self.decoder1 = DecoderBlock(256, 64, 64)\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, n_classes, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.final_up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "    def forward(self, x):\n",
        "        input_size = x.size()[2:]\n",
        "        x = self.firstconv(x)\n",
        "        x = self.firstbn(x)\n",
        "        x = self.firstrelu(x)\n",
        "        e0 = x\n",
        "        x = self.firstmaxpool(x)\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "        e4 = self.encoder4(e3)\n",
        "        e4 = self.center(e4)\n",
        "        d4 = self.decoder4(e4, e3)\n",
        "        d3 = self.decoder3(d4, e2)\n",
        "        d2 = self.decoder2(d3, e1)\n",
        "        d1 = self.decoder1(d2, e0)\n",
        "        out = self.final_conv(d1)\n",
        "        out = self.final_up(out)\n",
        "        if out.size()[2:] != input_size:\n",
        "            out = F.interpolate(out, size=input_size, mode='bilinear', align_corners=True)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e280936a",
      "metadata": {
        "id": "e280936a"
      },
      "outputs": [],
      "source": [
        "## Training Loop (unchanged)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    scheduler: optim.lr_scheduler._LRScheduler,\n",
        "    num_epochs: int,\n",
        "    device: torch.device\n",
        ") -> Tuple[List[float], List[float]]:\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        batch_count = 0\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            images = batch[\"image\"].to(device)\n",
        "            masks = batch[\"mask\"].to(device).float()\n",
        "\n",
        "            # Check for extra dimension and reshape if needed\n",
        "            if images.dim() == 5:  # If you have [batch, 1, channels, height, width]\n",
        "                images = images.squeeze(1)  # Remove the extra dimension -> [batch, channels, height, width]\n",
        "\n",
        "            # If masks also have an extra dimension\n",
        "            if masks.dim() == 4 and masks.shape[1] == 1:\n",
        "                masks = masks.squeeze(1)\n",
        "\n",
        "            # Check if masks are missing the channel dimension\n",
        "            if masks.dim() == 3:  # [batch, height, width]\n",
        "                masks = masks.unsqueeze(1)  # Add channel dimension: [batch, 1, height, width]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            if batch_idx == 0 and epoch == 0:\n",
        "                print(f\"Outputs: {outputs.shape}\\n\")\n",
        "                assert outputs.shape == masks.shape, \\\n",
        "                    f\"Shape mismatch: outputs {outputs.shape} vs masks {masks.shape}\"\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            batch_count += 1\n",
        "        train_loss = epoch_loss / batch_count\n",
        "        train_losses.append(train_loss)\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_count = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                images = batch[\"image\"].to(device)\n",
        "                masks = batch[\"mask\"].to(device).float()\n",
        "\n",
        "                # Fix dimensions\n",
        "                if images.dim() == 5:\n",
        "                    images = images.squeeze(1)\n",
        "\n",
        "                if masks.dim() == 3:\n",
        "                    masks = masks.unsqueeze(1)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item()\n",
        "                val_count += 1\n",
        "        val_loss = val_loss / val_count\n",
        "        val_losses.append(val_loss)\n",
        "        scheduler.step(val_loss)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}\")\n",
        "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        print(\"-\" * 40)\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P-0_JkB5tykD",
      "metadata": {
        "id": "P-0_JkB5tykD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1.):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # If logits, apply sigmoid\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "        # Flatten\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n",
        "        return 1 - dice\n",
        "\n",
        "class BCEDiceLoss(nn.Module):\n",
        "    def __init__(self, bce_weight=0.5, smooth=1.):\n",
        "        super().__init__()\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.bce_weight = bce_weight\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce = self.bce(inputs, targets)\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n",
        "        return self.bce_weight * bce + (1 - self.bce_weight) * (1 - dice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ee0732",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211,
          "referenced_widgets": [
            "d8e5e84abd9345a097f6173d451dcb81",
            "aa60a96bbbd64d15aad6001783f58a93",
            "54182efc5f5c469f95ee51f8fff42759",
            "fe6aa4e531c6464c96df6c8c80ce01e1",
            "54b6cb03454a4d2a947c28f3ece9e860",
            "1ce322f6a887416c92303e331eb7b259",
            "114f0b308d1f45e9b45c3d89c3c9810f",
            "422cac2a49fe40db9a124b2d3aa44fee",
            "2a5a23b08878427ea5493d068574465d",
            "5bcf15624ff84a2a83a265f99edfc048",
            "e1c3780e7f4949cd88290376b44be10f",
            "cf7297394d10464da1e0f466d84d5fbb",
            "a4df0dd46f224d85b87d6fa4db540bad",
            "741059b96f084a1691cde0c8a3357116",
            "2be69922ba3344e297088db72b5de62e",
            "ac0ace85f52d41b090ef828116df709c",
            "37f91deb20724b60b024845e8565f41d",
            "3e16852951104a76a2441e0c3f42254f",
            "36b05053e1834e2ebe4d06ca482ea15d",
            "76b6e97ae87d458a8896267d4a0380c9",
            "a36ebf70e3b846bc87814676223a2595",
            "e46bcb57530e4b63befb053c2ca7860d"
          ]
        },
        "id": "91ee0732",
        "outputId": "cebb069f-00d6-4658-c714-0c23824e6d12"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "model = smp.DeepLabV3Plus(\n",
        "    encoder_name=\"resnext50_32x4d\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    # encoder_weights=None,\n",
        "    in_channels=5,\n",
        "    classes=1\n",
        ")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21869584",
      "metadata": {
        "id": "21869584"
      },
      "source": [
        "## Main Training Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec06353f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ec06353f",
        "lines_to_next_cell": 1,
        "outputId": "e7ee4566-5fe3-4306-86ff-b050f5c0dc34"
      },
      "outputs": [],
      "source": [
        "!mkdir ../models\n",
        "# patch_size = 512  # defined earlier\n",
        "batch_size = 256\n",
        "num_epocs = 80\n",
        "\n",
        "naip_root = \"../cannabis-parcels/cannabis-parcels-masked\"\n",
        "annotations_dir = \"../cannabis-parcels/cannabis-parcels-masked\"\n",
        "train_transform, val_transform = get_transforms(patch_size)\n",
        "train_dataset = CannabisSegmentationGeoDataset(\n",
        "    naip_root=naip_root,\n",
        "    annotations_dir=annotations_dir,\n",
        "    transforms=train_transform\n",
        ")\n",
        "val_dataset = CannabisSegmentationGeoDataset(\n",
        "    naip_root=naip_root,\n",
        "    annotations_dir=annotations_dir,\n",
        "    transforms=val_transform\n",
        ")\n",
        "# Use RandomGeoSampler\n",
        "# Shared sampler parameters\n",
        "length = 1000\n",
        "pos_fraction = 0.9\n",
        "num_pos_per_poly = 10\n",
        "num_neg_per_image = 5\n",
        "\n",
        "train_sampler = PositiveNegativeGeoSampler(\n",
        "    train_dataset,\n",
        "    size=patch_size,\n",
        "    length=length,\n",
        "    pos_fraction=pos_fraction,\n",
        "    num_pos_per_poly=num_pos_per_poly,\n",
        "    num_neg_per_image=num_neg_per_image\n",
        ")\n",
        "\n",
        "val_sampler = PositiveNegativeGeoSampler(\n",
        "    val_dataset,\n",
        "    size=patch_size,\n",
        "    length=int(length * 0.2),  # 20% of train_sampler's length\n",
        "    pos_fraction=pos_fraction,\n",
        "    num_pos_per_poly=num_pos_per_poly,\n",
        "    num_neg_per_image=num_neg_per_image\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=collate_skip_none)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler, collate_fn=collate_skip_none)\n",
        "# Model, loss, optimizer (use new UNetWithResNet)\n",
        "# model = UNetWithResNet(n_channels=4, n_classes=1, pretrained=True).to(device)\n",
        "\n",
        "# Reload the already trained model\n",
        "# model = UNetWithResNet(n_channels=4, n_classes=1, pretrained=False)\n",
        "# model.load_state_dict(torch.load(\"../models/cannabis-segmentation-model.pth\", map_location=device))\n",
        "# model = model.to(device)\n",
        "# model.eval()\n",
        "\n",
        "# Criterion\n",
        "# criterion = nn.BCELoss()\n",
        "criterion = DiceLoss()\n",
        "criterion = BCEDiceLoss(bce_weight=0.5)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6, verbose=True)\n",
        "train_losses, val_losses = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=num_epocs,\n",
        "    device=device\n",
        ")\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "torch.save(model.state_dict(), \"../models/cannabis-segmentation-model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c81c6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "94c81c6a",
        "outputId": "f414d7d0-dfd3-48cb-e472-9e363ec62870"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def unnormalize(img, mean, std, bands=None):\n",
        "    \"\"\"\n",
        "    Unnormalize an image using mean and std\n",
        "    img: numpy array of shape (H,W,C) or (C,H,W)\n",
        "    mean, std: numpy arrays of shape (C,)\n",
        "    bands: tuple of indices to select (e.g., (0,1,2) for RGB)\n",
        "    \"\"\"\n",
        "    # Convert to HWC if in CHW format\n",
        "    if img.shape[0] == 3 or img.shape[0] == 4 or img.shape[0] == 5:  # CHW format\n",
        "        img = img.transpose(1, 2, 0)\n",
        "\n",
        "    # Select bands if specified\n",
        "    if bands is not None:\n",
        "        img = img[:, :, bands]\n",
        "        mean = mean[list(bands)]\n",
        "        std = std[list(bands)]\n",
        "\n",
        "    # Apply unnormalization\n",
        "    img = img * std + mean\n",
        "    return np.clip(img, 0, 1)\n",
        "\n",
        "def show_batch(images, masks, preds, num_samples=30):\n",
        "    \"\"\"\n",
        "    Simple display function for satellite imagery - no normalization\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(min(num_samples, len(images)), 3, figsize=(15, 5*min(num_samples, len(images))))\n",
        "\n",
        "    if num_samples == 1:\n",
        "        axs = axs.reshape(1, -1)\n",
        "\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        # Get RGB bands or first 3 bands if we have enough\n",
        "        img = images[i]\n",
        "\n",
        "        # Convert to HWC if in CHW format\n",
        "        if len(img.shape) == 3 and (img.shape[0] == 3 or img.shape[0] == 4 or img.shape[0] == 5):\n",
        "            img = img.transpose(1, 2, 0)\n",
        "\n",
        "        # Just take the first 3 bands for RGB display\n",
        "        if img.shape[-1] >= 3:\n",
        "            img_rgb = img[:, :, :3]\n",
        "        else:\n",
        "            # For fewer bands, duplicate the first band\n",
        "            img_rgb = np.repeat(img[:, :, 0:1], 3, axis=-1)\n",
        "\n",
        "        # Simple rescaling to [0,1] for display\n",
        "        img_min, img_max = img_rgb.min(), img_rgb.max()\n",
        "        if img_max > img_min:  # Avoid division by zero\n",
        "            img_rgb = (img_rgb - img_min) / (img_max - img_min)\n",
        "\n",
        "        # Display the images with auto-scaling\n",
        "        axs[i, 0].imshow(img_rgb)\n",
        "        axs[i, 0].set_title(\"Image (RGB)\")\n",
        "        axs[i, 0].axis(\"off\")\n",
        "\n",
        "        # Process masks and predictions - ensure they're 2D arrays\n",
        "        mask = masks[i]\n",
        "        pred = preds[i]\n",
        "\n",
        "        # Squeeze out single dimensions if present\n",
        "        if len(mask.shape) == 3 and mask.shape[0] == 1:\n",
        "            mask = mask[0]\n",
        "\n",
        "        if len(pred.shape) == 3 and pred.shape[0] == 1:\n",
        "            pred = pred[0]\n",
        "\n",
        "        # Display masks and predictions\n",
        "        axs[i, 1].imshow(mask, cmap='gray')\n",
        "        axs[i, 1].set_title(\"Ground Truth Mask\")\n",
        "        axs[i, 1].axis(\"off\")\n",
        "\n",
        "        axs[i, 2].imshow(pred, cmap='gray')\n",
        "        axs[i, 2].set_title(\"Predicted Mask\")\n",
        "        axs[i, 2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 1. Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. Load model\n",
        "model.load_state_dict(torch.load(\"../models/cannabis-segmentation-model.pth\", map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 3. Get a batch from your validation loader\n",
        "batch = next(iter(val_loader))\n",
        "images = batch[\"image\"].to(device)\n",
        "masks = batch[\"mask\"].to(device).float()\n",
        "\n",
        "# 4. Run inference\n",
        "with torch.no_grad():\n",
        "    # Fix the image dimensions before inference\n",
        "    if images.dim() == 5:  # If shape is [batch, 1, channels, height, width]\n",
        "        images = images.squeeze(1)  # Now shape should be [batch, channels, height, width]\n",
        "\n",
        "    outputs = model(images)\n",
        "    preds = (outputs > 0.5).float()\n",
        "\n",
        "# 5. Visualize using the helper function\n",
        "images_np = images.cpu().numpy()\n",
        "masks_np = masks.cpu().numpy()\n",
        "preds_np = preds.cpu().numpy()\n",
        "\n",
        "mean = np.array([0.485, 0.456, 0.406, 0.5, 0.0])  # Added 0.0 for NDVI\n",
        "std = np.array([0.229, 0.224, 0.225, 0.5, 1.0])   # Added 1.0 for NDVI\n",
        "\n",
        "show_batch(images_np, masks_np, preds_np)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "114f0b308d1f45e9b45c3d89c3c9810f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ce322f6a887416c92303e331eb7b259": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a5a23b08878427ea5493d068574465d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2be69922ba3344e297088db72b5de62e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a36ebf70e3b846bc87814676223a2595",
            "placeholder": "​",
            "style": "IPY_MODEL_e46bcb57530e4b63befb053c2ca7860d",
            "value": " 100M/100M [00:00&lt;00:00, 234MB/s]"
          }
        },
        "36b05053e1834e2ebe4d06ca482ea15d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37f91deb20724b60b024845e8565f41d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e16852951104a76a2441e0c3f42254f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "422cac2a49fe40db9a124b2d3aa44fee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54182efc5f5c469f95ee51f8fff42759": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_422cac2a49fe40db9a124b2d3aa44fee",
            "max": 134,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a5a23b08878427ea5493d068574465d",
            "value": 134
          }
        },
        "54b6cb03454a4d2a947c28f3ece9e860": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bcf15624ff84a2a83a265f99edfc048": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "741059b96f084a1691cde0c8a3357116": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36b05053e1834e2ebe4d06ca482ea15d",
            "max": 100417784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76b6e97ae87d458a8896267d4a0380c9",
            "value": 100417784
          }
        },
        "76b6e97ae87d458a8896267d4a0380c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a36ebf70e3b846bc87814676223a2595": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4df0dd46f224d85b87d6fa4db540bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37f91deb20724b60b024845e8565f41d",
            "placeholder": "​",
            "style": "IPY_MODEL_3e16852951104a76a2441e0c3f42254f",
            "value": "model.safetensors: 100%"
          }
        },
        "aa60a96bbbd64d15aad6001783f58a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ce322f6a887416c92303e331eb7b259",
            "placeholder": "​",
            "style": "IPY_MODEL_114f0b308d1f45e9b45c3d89c3c9810f",
            "value": "config.json: 100%"
          }
        },
        "ac0ace85f52d41b090ef828116df709c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf7297394d10464da1e0f466d84d5fbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4df0dd46f224d85b87d6fa4db540bad",
              "IPY_MODEL_741059b96f084a1691cde0c8a3357116",
              "IPY_MODEL_2be69922ba3344e297088db72b5de62e"
            ],
            "layout": "IPY_MODEL_ac0ace85f52d41b090ef828116df709c"
          }
        },
        "d8e5e84abd9345a097f6173d451dcb81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa60a96bbbd64d15aad6001783f58a93",
              "IPY_MODEL_54182efc5f5c469f95ee51f8fff42759",
              "IPY_MODEL_fe6aa4e531c6464c96df6c8c80ce01e1"
            ],
            "layout": "IPY_MODEL_54b6cb03454a4d2a947c28f3ece9e860"
          }
        },
        "e1c3780e7f4949cd88290376b44be10f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e46bcb57530e4b63befb053c2ca7860d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe6aa4e531c6464c96df6c8c80ce01e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bcf15624ff84a2a83a265f99edfc048",
            "placeholder": "​",
            "style": "IPY_MODEL_e1c3780e7f4949cd88290376b44be10f",
            "value": " 134/134 [00:00&lt;00:00, 9.32kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
