{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ac910d",
   "metadata": {},
   "source": [
    "# Cannabis Grow Segmentation with TorchGeo\n",
    "\n",
    "This notebook demonstrates how to train a semantic segmentation model to identify cannabis grows using NAIP imagery and polygon annotations. The model will learn to convert aerial imagery into binary masks that highlight cannabis cultivation areas.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Data**: We use NAIP (National Agriculture Imagery Program) aerial imagery and JSON polygon annotations\n",
    "2. **Model**: U-Net architecture for semantic segmentation\n",
    "3. **Training**: Binary cross-entropy loss with Adam optimizer\n",
    "\n",
    "## Setup\n",
    "First, let's import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41881006",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchgeo.datasets import NAIP\n",
    "from torchgeo.samplers import RandomGeoSampler\n",
    "from torchgeo.transforms import AugmentationSequential\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from shapely.geometry import shape, Polygon\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3335cd25",
   "metadata": {},
   "source": [
    "## Custom Dataset\n",
    "\n",
    "We create a custom PyTorch Dataset class that:\n",
    "1. Loads NAIP imagery using rasterio\n",
    "2. Converts JSON polygon annotations into binary masks\n",
    "3. Applies data augmentation transforms\n",
    "\n",
    "The dataset expects:\n",
    "- NAIP imagery in GeoTIFF format\n",
    "- Annotations in JSON format with polygon coordinates\n",
    "- Each JSON file should contain:\n",
    "  - `imagePath`: Path to corresponding NAIP image\n",
    "  - `imageHeight` and `imageWidth`: Image dimensions\n",
    "  - `shapes`: List of polygons with `label` and coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91ab6ee6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CannabisSegmentationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        naip_root: str,\n",
    "        annotations_dir: str,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.naip_root = Path(naip_root)\n",
    "        self.annotations_dir = Path(annotations_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get list of annotation files\n",
    "        self.annotation_files = list(self.annotations_dir.glob(\"*.json\"))\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.annotation_files)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # Load annotation file\n",
    "        with open(self.annotation_files[idx], \"r\") as f:\n",
    "            annotation = json.load(f)\n",
    "        \n",
    "        # Get corresponding NAIP image path\n",
    "        image_path = self.naip_root / annotation[\"imagePath\"]\n",
    "        \n",
    "        # Load NAIP image (4 channels)\n",
    "        with rasterio.open(image_path) as src:\n",
    "            image = src.read()  # This will read all 4 bands\n",
    "            image = np.transpose(image, (1, 2, 0))  # CHW -> HWC\n",
    "            \n",
    "        # Create mask from polygons\n",
    "        height, width = annotation[\"imageHeight\"], annotation[\"imageWidth\"]\n",
    "        mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        \n",
    "        for shape_data in annotation[\"shapes\"]:\n",
    "            if shape_data[\"label\"] == \"cannabis\":\n",
    "                # Convert points to a polygon\n",
    "                points = shape_data[\"points\"]\n",
    "                if len(points) >= 3:  # Need at least 3 points for a valid polygon\n",
    "                    polygon = Polygon(points)\n",
    "                    if polygon.is_valid:\n",
    "                        mask = rasterize(\n",
    "                            [polygon],\n",
    "                            out_shape=(height, width),\n",
    "                            fill=0,\n",
    "                            default_value=1,\n",
    "                            dtype=np.uint8\n",
    "                        )\n",
    "        \n",
    "        # Apply transforms if specified\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "        \n",
    "        # Make sure mask is float32 for BCE loss\n",
    "        mask = mask.float() if isinstance(mask, torch.Tensor) else torch.tensor(mask, dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"mask\": mask.unsqueeze(0)  # Add channel dimension for binary mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd92eb82",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "We define data augmentation transforms using the Albumentations library to:\n",
    "1. Increase the effective size of our training dataset\n",
    "2. Improve model generalization\n",
    "3. Make the model robust to variations in:\n",
    "   - Orientation (random rotations and flips)\n",
    "   - Lighting conditions (brightness/contrast)\n",
    "\n",
    "We also normalize the images using ImageNet mean and standard deviation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a79ccd24",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_transforms(image_size=(512, 512)):\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=image_size[0], width=image_size[1]),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        # Updated normalization for 4 channels (R,G,B,NIR)\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406, 0.5],  # Approximate NIR mean\n",
    "            std=[0.229, 0.224, 0.225, 0.225],  # Approximate NIR std\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=image_size[0], width=image_size[1]),\n",
    "        # Updated normalization for 4 channels (R,G,B,NIR)\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406, 0.5],  # Approximate NIR mean\n",
    "            std=[0.229, 0.224, 0.225, 0.225],  # Approximate NIR std\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5aac1f",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "\n",
    "The setup_model function initializes our U-Net model with a ResNet34 backbone:\n",
    "\n",
    "## 1. Encoder Path (Contracting):\n",
    "- Uses pretrained ResNet34 as the encoder backbone\n",
    "- ResNet34 provides strong feature extraction capabilities\n",
    "- Pretrained weights from ImageNet help with initialization\n",
    "- 4 levels of downsampling through ResNet34's layers\n",
    "- Each level doubles the number of channels\n",
    "- Each block: Conv2d -> BatchNorm -> ReLU -> Conv2d -> BatchNorm -> ReLU\n",
    "\n",
    "## 2. Decoder Path (Expanding):\n",
    "- 3 levels of upsampling\n",
    "- Skip connections from encoder to decoder\n",
    "- Each block: Transposed Conv2d -> Concatenate -> Double Conv\n",
    "\n",
    "## 3. Output:\n",
    "- 1x1 convolution to produce final segmentation map\n",
    "- Sigmoid activation for binary segmentation\n",
    "\n",
    "The model is designed for binary segmentation of cannabis grows in NAIP imagery, with the following configuration:\n",
    "- Input: 3 channels (RGB)\n",
    "- Output: 1 channel (binary mask)\n",
    "- Uses pretrained ResNet34 encoder for feature extraction\n",
    "- Uses skip connections to preserve spatial information\n",
    "- Implements batch normalization for stable training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec06353f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class UNetWithResNet(nn.Module):\n",
    "    def __init__(self, n_channels=4, n_classes=1, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet50\n",
    "        resnet = models.resnet50(weights='IMAGENET1K_V2' if pretrained else None)\n",
    "        \n",
    "        # Modify first conv layer to accept 4 channels\n",
    "        if n_channels != 3:\n",
    "            new_conv1 = nn.Conv2d(\n",
    "                n_channels, \n",
    "                64,\n",
    "                kernel_size=7, \n",
    "                stride=2, \n",
    "                padding=3,\n",
    "                bias=False\n",
    "            )\n",
    "            \n",
    "            # Initialize new conv1 with weights from pre-trained model\n",
    "            with torch.no_grad():\n",
    "                new_conv1.weight.zero_()\n",
    "                new_conv1.weight[:,:3,:,:] = resnet.conv1.weight\n",
    "                new_conv1.weight[:,3:,:,:] = resnet.conv1.weight.mean(dim=1, keepdim=True)\n",
    "            \n",
    "            resnet.conv1 = new_conv1\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.firstconv = resnet.conv1      # 64 channels\n",
    "        self.firstbn = resnet.bn1\n",
    "        self.firstrelu = resnet.relu\n",
    "        self.firstmaxpool = resnet.maxpool\n",
    "        self.encoder1 = resnet.layer1      # 256 channels\n",
    "        self.encoder2 = resnet.layer2      # 512 channels\n",
    "        self.encoder3 = resnet.layer3      # 1024 channels\n",
    "        self.encoder4 = resnet.layer4      # 2048 channels\n",
    "        \n",
    "        # Center convolution\n",
    "        self.center = nn.Sequential(\n",
    "            nn.Conv2d(2048, 2048, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder layers with correct channel numbers\n",
    "        self.decoder4 = DecoderBlock(2048, 1024, 1024)  # input: 2048+1024, output: 1024\n",
    "        self.decoder3 = DecoderBlock(1024, 512, 512)    # input: 1024+512, output: 512\n",
    "        self.decoder2 = DecoderBlock(512, 256, 256)     # input: 512+256, output: 256\n",
    "        self.decoder1 = DecoderBlock(256, 64, 64)       # input: 256+64, output: 64\n",
    "        \n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, n_classes, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Final upsampling to match input size\n",
    "        self.final_up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Save input size for final upsampling\n",
    "        input_size = x.size()[2:]\n",
    "        \n",
    "        # Initial convolution\n",
    "        x = self.firstconv(x)      # 64 channels\n",
    "        x = self.firstbn(x)\n",
    "        x = self.firstrelu(x)\n",
    "        e0 = x                     # Save for skip connection\n",
    "        x = self.firstmaxpool(x)\n",
    "        \n",
    "        # Encoder path\n",
    "        e1 = self.encoder1(x)      # 256 channels\n",
    "        e2 = self.encoder2(e1)     # 512 channels\n",
    "        e3 = self.encoder3(e2)     # 1024 channels\n",
    "        e4 = self.encoder4(e3)     # 2048 channels\n",
    "        \n",
    "        # Center\n",
    "        e4 = self.center(e4)\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        d4 = self.decoder4(e4, e3)  # 1024 channels\n",
    "        d3 = self.decoder3(d4, e2)  # 512 channels\n",
    "        d2 = self.decoder2(d3, e1)  # 256 channels\n",
    "        d1 = self.decoder1(d2, e0)  # 64 channels\n",
    "        \n",
    "        # Final convolution\n",
    "        out = self.final_conv(d1)\n",
    "        \n",
    "        # Final upsampling to match input size\n",
    "        out = self.final_up(out)\n",
    "        \n",
    "        # Ensure output size matches input size exactly\n",
    "        if out.size()[2:] != input_size:\n",
    "            out = F.interpolate(out, size=input_size, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels + skip_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        \n",
    "        # Handle different spatial dimensions\n",
    "        if x.shape[-2:] != skip.shape[-2:]:\n",
    "            x = F.interpolate(x, size=skip.shape[-2:], mode='bilinear', align_corners=True)\n",
    "            \n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46ccb4",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "The training function handles:\n",
    "1. Training and validation phases for each epoch\n",
    "2. Loss calculation and backpropagation\n",
    "3. Model evaluation\n",
    "4. Progress tracking\n",
    "\n",
    "We use:\n",
    "- Binary Cross-Entropy Loss: Appropriate for binary segmentation\n",
    "- Adam optimizer: Adaptive learning rate optimization\n",
    "- Learning rate of 1e-4: Typically good for segmentation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d52bf5eb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    scheduler: optim.lr_scheduler._LRScheduler,\n",
    "    num_epochs: int,\n",
    "    device: torch.device\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            masks = batch[\"mask\"].to(device)\n",
    "            \n",
    "            # Print shapes for debugging\n",
    "            if batch_idx == 0 and epoch == 0:\n",
    "                print(f\"\\nInput shapes:\")\n",
    "                print(f\"Images: {images.shape}\")\n",
    "                print(f\"Masks: {masks.shape}\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Print output shape for debugging\n",
    "            if batch_idx == 0 and epoch == 0:\n",
    "                print(f\"Outputs: {outputs.shape}\\n\")\n",
    "                \n",
    "                # Verify shapes match\n",
    "                assert outputs.shape == masks.shape, \\\n",
    "                    f\"Shape mismatch: outputs {outputs.shape} vs masks {masks.shape}\"\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        train_loss = epoch_loss / batch_count\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch[\"image\"].to(device)\n",
    "                masks = batch[\"mask\"].to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                val_count += 1\n",
    "        \n",
    "        val_loss = val_loss / val_count\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c804fce",
   "metadata": {},
   "source": [
    "## Main Training Script\n",
    "\n",
    "This section puts everything together:\n",
    "1. Sets up the device (GPU if available)\n",
    "2. Configures data paths and transforms\n",
    "3. Creates datasets and data loaders\n",
    "4. Initializes the model, loss function, and optimizer\n",
    "5. Trains the model\n",
    "6. Visualizes training progress\n",
    "7. Saves the trained model\n",
    "\n",
    "Note: Adjust the paths (`naip_root` and `annotations_dir`) to match your data locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d5b11da",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set paths\n",
    "    naip_root = \"../cannabis-parcels/cannabis-parcels-masked\"\n",
    "    annotations_dir = \"../cannabis-parcels/cannabis-parcels-masked\"\n",
    "    \n",
    "    # Set image size - smaller size to handle memory constraints\n",
    "    image_size = (512, 512)\n",
    "    \n",
    "    # Get transforms\n",
    "    train_transform, val_transform = get_transforms(image_size=image_size)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CannabisSegmentationDataset(\n",
    "        naip_root=naip_root,\n",
    "        annotations_dir=annotations_dir,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = CannabisSegmentationDataset(\n",
    "        naip_root=naip_root,\n",
    "        annotations_dir=annotations_dir,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    \n",
    "    # Create data loaders with smaller batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)  # Reduced from 4\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2)  # Reduced from 4\n",
    "    \n",
    "    # Initialize model with pre-trained ResNet backbone\n",
    "    model = UNetWithResNet(\n",
    "        n_channels=4,\n",
    "        n_classes=1,\n",
    "        pretrained=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Set up loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)  # Reduced learning rate\n",
    "    \n",
    "    # Add learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    train_losses, val_losses = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,  # Add scheduler to training\n",
    "        num_epochs=50,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"cannabis_segmentation_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c3bb30b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wxmiked/vscode-workspace/cannabis/cannabis-deforestation/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input shapes:\n",
      "Images: torch.Size([2, 4, 512, 512])\n",
      "Masks: torch.Size([2, 1, 512, 512])\n",
      "Outputs: torch.Size([2, 1, 512, 512])\n",
      "\n",
      "Epoch 1/50\n",
      "Train Loss: 0.7362\n",
      "Val Loss: 0.7259\n",
      "Learning Rate: 0.000010\n",
      "----------------------------------------\n",
      "Epoch 2/50\n",
      "Train Loss: 0.7208\n",
      "Val Loss: 0.7230\n",
      "Learning Rate: 0.000010\n",
      "----------------------------------------\n",
      "Epoch 3/50\n",
      "Train Loss: 0.7084\n",
      "Val Loss: 0.7001\n",
      "Learning Rate: 0.000010\n",
      "----------------------------------------\n",
      "Epoch 4/50\n",
      "Train Loss: 0.6934\n",
      "Val Loss: 0.6870\n",
      "Learning Rate: 0.000010\n",
      "----------------------------------------\n",
      "Epoch 5/50\n",
      "Train Loss: 0.6692\n",
      "Val Loss: 0.6726\n",
      "Learning Rate: 0.000010\n",
      "----------------------------------------\n",
      "Epoch 6/50\n",
      "Train Loss: 0.6422\n",
      "Val Loss: 0.6552\n",
      "Learning Rate: 0.000010\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[82], line 54\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(\n\u001b[1;32m     46\u001b[0m     optimizer, \n\u001b[1;32m     47\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add scheduler to training\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Plot training curves\u001b[39;00m\n\u001b[1;32m     66\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[73], line 43\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, masks)\n\u001b[1;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 43\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     46\u001b[0m batch_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/vscode-workspace/cannabis/cannabis-deforestation/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/vscode-workspace/cannabis/cannabis-deforestation/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/vscode-workspace/cannabis/cannabis-deforestation/.venv/lib/python3.11/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/vscode-workspace/cannabis/cannabis-deforestation/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vscode-workspace/cannabis/cannabis-deforestation/.venv/lib/python3.11/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vscode-workspace/cannabis/cannabis-deforestation/.venv/lib/python3.11/site-packages/torch/optim/adam.py:379\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    378\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 379\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    382\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
