{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "04ac910d",
      "metadata": {
        "id": "04ac910d"
      },
      "source": [
        "# Cannabis Grow Segmentation with TorchGeo (torchgeo-compatible version)\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wxmiked/cannabis-deforestation/blob/main/notebooks/cannabis-segmentation-torchgeo.ipynb)\n",
        "\n",
        "This script refactors the dataset and dataloaders to use torchgeo's GeoDataset and RandomGeoSampler."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchgeo"
      ],
      "metadata": {
        "id": "nuyXOeXFjK8c",
        "outputId": "2e6a992f-b146-4462-ae61-60f2f1162ed7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nuyXOeXFjK8c",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchgeo in /usr/local/lib/python3.11/dist-packages (0.7.0)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (0.8.1)\n",
            "Requirement already satisfied: fiona>=1.8.22 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (1.10.1)\n",
            "Requirement already satisfied: kornia>=0.7.4 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (0.8.0)\n",
            "Requirement already satisfied: lightly!=1.4.26,>=1.4.5 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (1.5.20)\n",
            "Requirement already satisfied: lightning!=2.3.*,!=2.5.0,>=2 in /usr/local/lib/python3.11/dist-packages (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (2.5.1.post0)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.2 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (11.1.0)\n",
            "Requirement already satisfied: pyproj>=3.4 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (3.7.1)\n",
            "Requirement already satisfied: rasterio!=1.4.0,!=1.4.1,!=1.4.2,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (1.4.3)\n",
            "Requirement already satisfied: rtree>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (1.4.0)\n",
            "Requirement already satisfied: segmentation-models-pytorch>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (0.5.0)\n",
            "Requirement already satisfied: shapely>=1.8.5 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (2.1.0)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (1.0.15)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchmetrics>=1.2 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (1.7.1)\n",
            "Requirement already satisfied: torchvision>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (0.21.0+cu124)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (4.13.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.22->torchgeo) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.22->torchgeo) (2025.1.31)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.22->torchgeo) (8.1.8)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.22->torchgeo) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.22->torchgeo) (0.7.2)\n",
            "Requirement already satisfied: kornia_rs>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from kornia>=0.7.4->torchgeo) (0.1.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kornia>=0.7.4->torchgeo) (24.2)\n",
            "Requirement already satisfied: hydra-core>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (1.3.2)\n",
            "Requirement already satisfied: lightly_utils~=0.0.0 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (0.0.2)\n",
            "Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.32.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (1.17.0)\n",
            "Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (4.67.1)\n",
            "Requirement already satisfied: pydantic>=1.10.5 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.11.3)\n",
            "Requirement already satisfied: pytorch_lightning>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.5.1.post0)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.3.0)\n",
            "Requirement already satisfied: aenum>=3.1.11 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (3.1.16)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (2025.3.2)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (0.14.3)\n",
            "Requirement already satisfied: jsonargparse<5.0,>=4.27.7 in /usr/local/lib/python3.11/dist-packages (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (4.38.0)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (2.3.0)\n",
            "Requirement already satisfied: rich<14.0,>=12.3.0 in /usr/local/lib/python3.11/dist-packages (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (13.9.4)\n",
            "Requirement already satisfied: tensorboardX<3.0,>=2.2 in /usr/local/lib/python3.11/dist-packages (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (2.6.2.2)\n",
            "Requirement already satisfied: bitsandbytes<1.0,>=0.45.2 in /usr/local/lib/python3.11/dist-packages (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (0.45.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->torchgeo) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->torchgeo) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->torchgeo) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->torchgeo) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->torchgeo) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->torchgeo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->torchgeo) (2025.2)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.11/dist-packages (from rasterio!=1.4.0,!=1.4.1,!=1.4.2,>=1.3.3->torchgeo) (2.4.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch>=0.3.3->torchgeo) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch>=0.3.3->torchgeo) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2->torchgeo) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (3.11.15)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.0.0->lightly!=1.4.26,>=1.4.5->torchgeo) (4.9.3)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (0.16)\n",
            "Requirement already satisfied: typeshed-client>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (2.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (75.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->lightly!=1.4.26,>=1.4.5->torchgeo) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->lightly!=1.4.26,>=1.4.5->torchgeo) (3.10)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (2.18.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX<3.0,>=2.2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (5.29.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchgeo) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (1.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (0.1.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (6.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/wxmiked/cannabis-deforestation.git\n",
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "id": "n7f7mQ7kkybV",
        "outputId": "b6946a6a-967c-4f35-efcb-766b9738a571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "n7f7mQ7kkybV",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'cannabis-deforestation' already exists and is not an empty directory.\n",
            "/content\n",
            "cannabis-deforestation\tcannabis_segmentation_model.pth  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41881006",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "41881006"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchgeo.datasets import GeoDataset\n",
        "from torchgeo.samplers import RandomGeoSampler\n",
        "from torchgeo.transforms import AugmentationSequential\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "from shapely.geometry import shape, box\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from rasterio.crs import CRS\n",
        "from rasterio.transform import xy\n",
        "from shapely.geometry import Polygon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e641ecf",
      "metadata": {
        "id": "5e641ecf"
      },
      "outputs": [],
      "source": [
        "# ────────── Cell: Fix CannabisSegmentationGeoDataset ──────────\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "from shapely.geometry import Polygon\n",
        "from torchgeo.datasets import RasterDataset\n",
        "from rtree import index as rindex\n",
        "import re\n",
        "from datetime import datetime\n",
        "from rasterio.windows import Window\n",
        "from rasterio.features import rasterize\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "class CannabisSegmentationGeoDataset(RasterDataset):\n",
        "    def __init__(self, annotations_dir, naip_root, transforms=None):\n",
        "        self.annotation_paths = sorted(Path(annotations_dir).glob(\"*.json\"))\n",
        "        self.naip_root = Path(naip_root)\n",
        "        self.filename_glob = \"*_*.tif\"\n",
        "        self.filename_regex = r\"^.*_(?P<date>\\d{8})\"\n",
        "        self.date_format = \"%Y%m%d\"\n",
        "        self.is_image = True\n",
        "        self.separate_files = False\n",
        "        self.all_bands = (\"R\", \"G\", \"B\", \"NIR\")\n",
        "        self.rgb_bands = (\"R\", \"G\", \"B\",)\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # Build a mapping from ann_path → image_path\n",
        "        self.image_paths = {}\n",
        "        for ann_path in self.annotation_paths:\n",
        "            ann = json.load(open(ann_path))\n",
        "            self.image_paths[ann_path] = self.naip_root / ann[\"imagePath\"]\n",
        "\n",
        "        super().__init__(paths=self.image_paths.values(), transforms=transforms)\n",
        "\n",
        "        # 1) compute new_bounds\n",
        "        self._bounds = []\n",
        "        for ann_path, img_path in self.image_paths.items():\n",
        "            with rasterio.open(img_path) as src:\n",
        "                self._crs = src.crs\n",
        "                b = src.bounds\n",
        "                self._bounds.append((b.left, b.bottom, b.right, b.top))\n",
        "\n",
        "        # 3) set the dates\n",
        "        dates = []\n",
        "        for fname in self.image_paths.values():\n",
        "            match = re.match(r\"^.*_(\\d{8})\", fname.name)\n",
        "            if match:\n",
        "                date = datetime.strptime(match.group(1), \"%Y%m%d\")\n",
        "                dates.append(date.timestamp())\n",
        "        if dates:\n",
        "            self.mint = min(dates)\n",
        "            self.maxt = max(dates)\n",
        "        else:\n",
        "            self.mint = 0\n",
        "            self.maxt = 9223372036854775807\n",
        "\n",
        "        # 4) update bounds with mint and maxt\n",
        "        self._bounds = [(b[0], b[1], b[2], b[3], self.mint, self.maxt) for b in self._bounds]\n",
        "\n",
        "        # 2) rebuild index by hand\n",
        "        p = rindex.Property()\n",
        "        p.dimension = 3\n",
        "        self.index = rindex.Index(properties=p)\n",
        "        for i, b in enumerate(self._bounds):\n",
        "            self.index.insert(i, b)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotation_paths)\n",
        "\n",
        "    def __getitem__(self, query):\n",
        "        # 1) unpack\n",
        "        size = query.get(\"size\")\n",
        "        if size is None:\n",
        "            raise ValueError(\"Query dict must include 'size'\")\n",
        "        bbox = query[\"bbox\"]                      # (minx, miny, maxx, maxy)\n",
        "\n",
        "        # 2) 3-D vs 2-D R-tree lookup\n",
        "        bbox = tuple(float(x) for x in bbox)\n",
        "        if self.index.properties.dimension == 3:\n",
        "            lookup = (bbox[0], bbox[1], self.mint, bbox[2], bbox[3], self.maxt)\n",
        "        else:\n",
        "            lookup = bbox\n",
        "        idxs = list(self.index.intersection(lookup))\n",
        "        if not idxs:\n",
        "            raise ValueError(f\"No annotation intersects bbox {bbox}\")\n",
        "        ann_path = self.annotation_paths[idxs[0]]\n",
        "        img_path = self.image_paths[ann_path]\n",
        "\n",
        "        # 3) open & read fixed window\n",
        "        with rasterio.open(img_path) as src:\n",
        "            # center in world → pixel\n",
        "            cx, cy = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n",
        "            rowc, colc = src.index(cx, cy)\n",
        "\n",
        "            # Clamp window so it stays within raster bounds\n",
        "            half = size // 2\n",
        "            col_off = int(np.clip(colc - half, 0, src.width  - size))\n",
        "            row_off = int(np.clip(rowc - half, 0, src.height - size))\n",
        "            window  = Window(col_off, row_off, size, size)\n",
        "\n",
        "            # Read image with this window\n",
        "            image = src.read(window=window)\n",
        "            _, actual_height, actual_width = image.shape\n",
        "\n",
        "            # build mask geometries\n",
        "            ann = json.load(open(ann_path))\n",
        "            shapes = [\n",
        "                (labelme_points_to_geopolygon(s[\"points\"], src.transform), 1)\n",
        "                for s in ann[\"shapes\"] if s[\"label\"] != \"cannabis - old\"\n",
        "            ]\n",
        "\n",
        "            # Get the transform for the ACTUAL window\n",
        "            actual_transform = src.window_transform(window)\n",
        "\n",
        "            # print(f\"col_off: {col_off}, row_off: {row_off}, window: {window}\")\n",
        "            # print(f\"actual_transform: {actual_transform}\")\n",
        "\n",
        "            # Rasterize mask with the actual transform and shape\n",
        "            mask = rasterize(\n",
        "                        shapes,\n",
        "                        out_shape=(actual_height, actual_width),\n",
        "                        transform=actual_transform,\n",
        "                        fill=0,\n",
        "                        dtype=\"uint8\",\n",
        "                    )[None]\n",
        "\n",
        "            assert image.shape[1:] == mask.shape[1:], f\"Image and mask shapes do not match: {image.shape}, {mask.shape}\"\n",
        "\n",
        "        # 4) Albumentations / ToTensor\n",
        "        if self.transforms:\n",
        "            img_np = np.transpose(image, (1, 2, 0))    # HxWxC\n",
        "            data   = self.transforms(image=img_np, mask=mask[0])\n",
        "            image  = data[\"image\"]                     # tensor (C,H,W)\n",
        "            mask   = data[\"mask\"].unsqueeze(0).float()         # (1,H,W)\n",
        "\n",
        "        return {\"image\": image, \"mask\": mask}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e85659e",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "5e85659e"
      },
      "outputs": [],
      "source": [
        "# ────────── Sample one patch manually ──────────\n",
        "\n",
        "import json, numpy as np, rasterio, matplotlib.pyplot as plt\n",
        "from rasterio.transform import xy\n",
        "from rasterio.features import rasterize\n",
        "from shapely.geometry import Polygon\n",
        "\n",
        "# 1) tranformation for labelme images to a coordinate reference system\n",
        "def labelme_points_to_geopolygon(points, transform):\n",
        "    return Polygon([xy(transform, y, x) for x, y in points])\n",
        "\n",
        "# 2) Instantiate dataset (make sure you've run the fixed class definition)\n",
        "dataset = CannabisSegmentationGeoDataset(\n",
        "    annotations_dir=\"cannabis-deforestation/cannabis-parcels/cannabis-parcels-masked\",\n",
        "    naip_root=\"cannabis-deforestation/cannabis-parcels/cannabis-parcels-masked\",\n",
        "    transforms=None\n",
        ")\n",
        "\n",
        "# 3) Grab the first annotation file + JSON\n",
        "ann_path = dataset.annotation_paths[4]\n",
        "ann = json.load(open(ann_path))\n",
        "img_path = dataset.image_paths[ann_path]\n",
        "\n",
        "# 4) Open the NAIP tile and get the first \"cannabis\" polygon\n",
        "with rasterio.open(img_path) as src:\n",
        "    image = src.read()\n",
        "    win_t = src.transform\n",
        "\n",
        "# 5) Rasterize the mask for all cannabis shapes in that window\n",
        "_, h, w = image.shape\n",
        "mask = np.zeros((h, w), dtype=np.uint8)\n",
        "for shape in ann[\"shapes\"]:\n",
        "    if shape[\"label\"] != \"cannabis - old\":\n",
        "        p = labelme_points_to_geopolygon(shape[\"points\"], src.transform)\n",
        "\n",
        "        # Print info to debug\n",
        "        print(\"Polygon bounds:\", p.bounds)\n",
        "        print(\"Do they intersect?\", p.intersects(box(*src.bounds)))\n",
        "\n",
        "        # Clip the polygon to the window\n",
        "        p_window = p.intersection(box(*src.bounds))\n",
        "\n",
        "        print(\"Image shape (bands, height, width):\", image.shape)\n",
        "        print(\"Rasterizing to shape:\", (h, w))\n",
        "\n",
        "        # Proceed only if there's something to rasterize\n",
        "        if not p_window.is_empty:\n",
        "            single = rasterize(\n",
        "                [(p_window, 1)],\n",
        "                out_shape=(h, w),\n",
        "                transform=win_t,\n",
        "                all_touched=True,\n",
        "                dtype=np.uint8\n",
        "            )\n",
        "            print(\"Single shape max:\", np.max(single))\n",
        "            mask = np.maximum(mask, single)\n",
        "            print(\"Mask max:\", np.max(mask))\n",
        "        else:\n",
        "            print(\"Polygon is empty after intersection\")\n",
        "\n",
        "# 6) Plot\n",
        "if image.ndim == 3 and image.shape[0] in [3, 4]:\n",
        "    image = np.transpose(image, (1, 2, 0))\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image[..., :3], extent=src.bounds)\n",
        "plt.title(\"Image Patch\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(mask, cmap=\"Reds\", alpha=0.6, extent=src.bounds)\n",
        "plt.title(\"Cannabis Mask\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9b549c",
      "metadata": {
        "id": "5c9b549c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# image: shape (H, W, C), mask: shape (H, W)\n",
        "# If image is CHW, convert to HWC for plotting\n",
        "if image.shape[0] in [3, 4] and image.shape[0] < image.shape[1]:\n",
        "    image = np.transpose(image, (1, 2, 0))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image[..., :3])  # Show only RGB channels if image has 4 channels\n",
        "plt.contour(mask, colors='#FF00FF', linewidths=1, alpha=0.3)  # Outline mask in bright purple, adjust linewidth as needed\n",
        "plt.title(\"Image with Mask Outline\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bb5d262",
      "metadata": {
        "id": "0bb5d262"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import rasterio\n",
        "\n",
        "# Delete [:1] in the line below to get the full dataset\n",
        "for ann_path in dataset.annotation_paths[:1]:\n",
        "    print(ann_path)\n",
        "    ann = json.load(open(ann_path))\n",
        "    img_path = dataset.naip_root / ann[\"imagePath\"]\n",
        "\n",
        "    with rasterio.open(img_path) as src:\n",
        "        image = src.read()  # Read full image\n",
        "        h, w = image.shape[1:]\n",
        "        # Rasterize full mask\n",
        "        shapes = [\n",
        "            (labelme_points_to_geopolygon(s[\"points\"], src.transform), 1)\n",
        "            for s in ann[\"shapes\"] if s[\"label\"] != \"cannabis - old\"\n",
        "        ]\n",
        "        mask = rasterio.features.rasterize(\n",
        "            shapes,\n",
        "            out_shape=(h, w),\n",
        "            transform=src.transform,\n",
        "            fill=0,\n",
        "            dtype=\"uint8\",\n",
        "        )\n",
        "\n",
        "    # Convert image to HWC for plotting if needed\n",
        "    if image.shape[0] in [3, 4] and image.shape[0] < image.shape[1]:\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image[..., :3])\n",
        "    plt.contour(mask, colors='#FF00FF', linewidths=1, alpha=0.3)\n",
        "    plt.title(f\"{ann_path.name} with Mask Outline\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b48ebbe",
      "metadata": {
        "id": "9b48ebbe"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "random.seed(42)      # or any integer you like\n",
        "np.random.seed(42)   # if you use numpy random anywhere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65cd337a",
      "metadata": {
        "id": "65cd337a"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "import json, random, rasterio\n",
        "from rasterio.transform import xy\n",
        "from shapely.geometry import Polygon, box\n",
        "from torchgeo.samplers import RandomGeoSampler\n",
        "\n",
        "def labelme_points_to_geopolygon(points, transform):\n",
        "    return Polygon([xy(transform, y, x) for x, y in points])\n",
        "\n",
        "class PositiveNegativeGeoSampler(RandomGeoSampler):\n",
        "    def __init__(self, dataset, size, length, pos_fraction=0.5):\n",
        "        super().__init__(dataset, size, length)\n",
        "        self.ds = dataset\n",
        "        self.size = size\n",
        "        self.length = length\n",
        "        self.pos_frac = pos_fraction\n",
        "        self.pos_bboxes: List[Tuple[float,float,float,float]] = []\n",
        "        self.neg_bboxes: List[Tuple[float,float,float,float]] = []\n",
        "\n",
        "        for ann_path in self.ds.annotation_paths:\n",
        "            ann = json.load(open(ann_path))\n",
        "            img_path = self.ds.image_paths[ann_path]\n",
        "            with rasterio.open(img_path) as src:\n",
        "                left, bottom, right, top = src.bounds\n",
        "                rx, ry = src.res\n",
        "                half_w = size/2 * rx\n",
        "                half_h = size/2 * abs(ry)\n",
        "                polys = []\n",
        "\n",
        "                # positives: only if full window fits\n",
        "                for shape_ in ann[\"shapes\"]:\n",
        "                    if shape_[\"label\"] != \"cannabis\":\n",
        "                        continue\n",
        "                    poly = labelme_points_to_geopolygon(shape_[\"points\"], src.transform)\n",
        "                    polys.append(poly)\n",
        "                    cx, cy = poly.centroid.x, poly.centroid.y\n",
        "                    pb = (cx-half_w, cy-half_h, cx+half_w, cy+half_h)\n",
        "                    if pb[0] >= left and pb[2] <= right and pb[1] >= bottom and pb[3] <= top:\n",
        "                        self.pos_bboxes.append(pb)\n",
        "\n",
        "                    # print(\"Patch bbox:\", pb)\n",
        "                    # print(\"Image bounds:\", left, bottom, right, top)\n",
        "                    # print(\"Patch width:\", pb[2] - pb[0], \"Patch height:\", pb[3] - pb[1])\n",
        "                    # print(\"Image width:\", right - left, \"Image height:\", top - bottom)\n",
        "\n",
        "                # negatives: random windows fully inside & not overlapping\n",
        "                w, h = src.width, src.height\n",
        "                for _ in range(max(1, len(polys)*2)):\n",
        "                    for __ in range(50):\n",
        "                        ix, iy = random.randint(0, w-1), random.randint(0, h-1)\n",
        "                        cx2, cy2 = xy(src.transform, iy, ix)\n",
        "                        nb = (cx2-half_w, cy2-half_h, cx2+half_w, cy2+half_h)\n",
        "                        if (\n",
        "                            nb[0] >= left and nb[2] <= right and\n",
        "                            nb[1] >= bottom and nb[3] <= top and\n",
        "                            not any(p.intersects(box(*nb)) for p in polys)\n",
        "                        ):\n",
        "                            self.neg_bboxes.append(nb)\n",
        "                            break\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.length):\n",
        "            if random.random() < self.pos_frac and self.pos_bboxes:\n",
        "                print(\"+\", end=\"\", flush=True)\n",
        "                bbox = random.choice(self.pos_bboxes)\n",
        "            else:\n",
        "                print(\"-\", end=\"\", flush=True)\n",
        "                bbox = random.choice(self.neg_bboxes)\n",
        "            yield {\"bbox\": bbox, \"size\": self.size}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "377b65bc",
      "metadata": {
        "id": "377b65bc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bceecff2",
      "metadata": {
        "id": "bceecff2"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ─── Usage ───\n",
        "patch_size = 256\n",
        "sampler = PositiveNegativeGeoSampler(dataset, size=patch_size,\n",
        "                                     length=10, pos_fraction=0.5)\n",
        "loader = DataLoader(dataset, batch_size=8, sampler=sampler)\n",
        "\n",
        "# Visualize one batch\n",
        "batch = next(iter(loader))\n",
        "imgs, msks = batch[\"image\"], batch[\"mask\"]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(len(imgs)):\n",
        "    img = imgs[i].cpu().numpy()\n",
        "    if img.ndim==3 and img.shape[0] in [3,4]:\n",
        "        img = img.transpose(1,2,0)\n",
        "    msk = msks[i,0].cpu().numpy()   # pick channel 0 → shape (H,W)\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(img[..., :3]); plt.title(\"Image\"); plt.axis(\"off\")\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(msk, cmap=\"gray\"); plt.title(\"Mask\"); plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd92eb82",
      "metadata": {
        "id": "dd92eb82"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a79ccd24",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "a79ccd24"
      },
      "outputs": [],
      "source": [
        "def get_transforms(patch_size):\n",
        "    train_transform = A.Compose([\n",
        "        A.RandomCrop(patch_size, patch_size),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.2),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    val_transform = A.Compose([\n",
        "        A.CenterCrop(patch_size, patch_size),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "    return train_transform, val_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bbf5c3f",
      "metadata": {
        "id": "9bbf5c3f"
      },
      "outputs": [],
      "source": [
        "## U-Net Model Architecture (unchanged)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, skip_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(out_channels + skip_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        if x.shape[-2:] != skip.shape[-2:]:\n",
        "            x = F.interpolate(x, size=skip.shape[-2:], mode='bilinear', align_corners=True)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class UNetWithResNet(nn.Module):\n",
        "    def __init__(self, n_channels=4, n_classes=1, pretrained=True):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet50(weights='IMAGENET1K_V2' if pretrained else None)\n",
        "        # Modify first conv layer to accept 4 channels\n",
        "        w = resnet.conv1.weight\n",
        "        resnet.conv1 = nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        if n_channels == 4:\n",
        "            resnet.conv1.weight.data[:, :3, :, :] = w.data\n",
        "            resnet.conv1.weight.data[:, 3:, :, :].zero_()\n",
        "        self.firstconv = resnet.conv1\n",
        "        self.firstbn = resnet.bn1\n",
        "        self.firstrelu = resnet.relu\n",
        "        self.firstmaxpool = resnet.maxpool\n",
        "        self.encoder1 = resnet.layer1\n",
        "        self.encoder2 = resnet.layer2\n",
        "        self.encoder3 = resnet.layer3\n",
        "        self.encoder4 = resnet.layer4\n",
        "        self.center = nn.Sequential(\n",
        "            nn.Conv2d(2048, 2048, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(2048),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.decoder4 = DecoderBlock(2048, 1024, 1024)\n",
        "        self.decoder3 = DecoderBlock(1024, 512, 512)\n",
        "        self.decoder2 = DecoderBlock(512, 256, 256)\n",
        "        self.decoder1 = DecoderBlock(256, 64, 64)\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, n_classes, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.final_up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "    def forward(self, x):\n",
        "        input_size = x.size()[2:]\n",
        "        x = self.firstconv(x)\n",
        "        x = self.firstbn(x)\n",
        "        x = self.firstrelu(x)\n",
        "        e0 = x\n",
        "        x = self.firstmaxpool(x)\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(e1)\n",
        "        e3 = self.encoder3(e2)\n",
        "        e4 = self.encoder4(e3)\n",
        "        e4 = self.center(e4)\n",
        "        d4 = self.decoder4(e4, e3)\n",
        "        d3 = self.decoder3(d4, e2)\n",
        "        d2 = self.decoder2(d3, e1)\n",
        "        d1 = self.decoder1(d2, e0)\n",
        "        out = self.final_conv(d1)\n",
        "        out = self.final_up(out)\n",
        "        if out.size()[2:] != input_size:\n",
        "            out = F.interpolate(out, size=input_size, mode='bilinear', align_corners=True)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e280936a",
      "metadata": {
        "id": "e280936a"
      },
      "outputs": [],
      "source": [
        "## Training Loop (unchanged)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    scheduler: optim.lr_scheduler._LRScheduler,\n",
        "    num_epochs: int,\n",
        "    device: torch.device\n",
        ") -> Tuple[List[float], List[float]]:\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        batch_count = 0\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            images = batch[\"image\"].to(device)\n",
        "            masks = batch[\"mask\"].to(device).float()\n",
        "            if batch_idx == 0 and epoch == 0:\n",
        "                print(f\"\\nInput shapes:\")\n",
        "                print(f\"Images: {images.shape}\")\n",
        "                print(f\"Masks: {masks.shape}\")\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            if batch_idx == 0 and epoch == 0:\n",
        "                print(f\"Outputs: {outputs.shape}\\n\")\n",
        "                assert outputs.shape == masks.shape, \\\n",
        "                    f\"Shape mismatch: outputs {outputs.shape} vs masks {masks.shape}\"\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            batch_count += 1\n",
        "        train_loss = epoch_loss / batch_count\n",
        "        train_losses.append(train_loss)\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_count = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                images = batch[\"image\"].to(device)\n",
        "                masks = batch[\"mask\"].to(device).float()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item()\n",
        "                val_count += 1\n",
        "        val_loss = val_loss / val_count\n",
        "        val_losses.append(val_loss)\n",
        "        scheduler.step(val_loss)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}\")\n",
        "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        print(\"-\" * 40)\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21869584",
      "metadata": {
        "id": "21869584"
      },
      "source": [
        "## Main Training Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec06353f",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ec06353f"
      },
      "outputs": [],
      "source": [
        "patch_size = 256  # or whatever your crop size is\n",
        "batch_size = 64\n",
        "num_epocs = 200\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "naip_root = \"cannabis-deforestation/cannabis-parcels/cannabis-parcels-masked\"\n",
        "annotations_dir = \"cannabis-deforestation/cannabis-parcels/cannabis-parcels-masked\"\n",
        "train_transform, val_transform = get_transforms(patch_size)\n",
        "train_dataset = CannabisSegmentationGeoDataset(\n",
        "    naip_root=naip_root,\n",
        "    annotations_dir=annotations_dir,\n",
        "    transforms=train_transform\n",
        ")\n",
        "val_dataset = CannabisSegmentationGeoDataset(\n",
        "    naip_root=naip_root,\n",
        "    annotations_dir=annotations_dir,\n",
        "    transforms=val_transform\n",
        ")\n",
        "# Use RandomGeoSampler\n",
        "train_sampler = PositiveNegativeGeoSampler(train_dataset, size=patch_size, length=25)  # adjust length as needed\n",
        "val_sampler = PositiveNegativeGeoSampler(val_dataset, size=patch_size, length=11)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "# Model, loss, optimizer (use new UNetWithResNet)\n",
        "model = UNetWithResNet(n_channels=4, n_classes=1, pretrained=True).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "train_losses, val_losses = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=num_epocs,\n",
        "    device=device\n",
        ")\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "torch.save(model.state_dict(), \"cannabis_segmentation_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c81c6a",
      "metadata": {
        "id": "94c81c6a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def unnormalize(img, mean, std):\n",
        "    \"\"\"Unnormalize a CHW or HWC image array.\"\"\"\n",
        "    img = img.copy()\n",
        "    if img.shape[-1] == len(mean):  # HWC\n",
        "        img = img * std + mean\n",
        "    else:  # CHW\n",
        "        img = (img.transpose(1, 2, 0) * std + mean)\n",
        "    return np.clip(img, 0, 1)\n",
        "\n",
        "def show_batch(images, masks, preds=None, mean=None, std=None, bands=(0,1,2)):\n",
        "    \"\"\"\n",
        "    Visualize a batch of images, masks, and (optionally) predictions.\n",
        "    images: numpy array (B, C, H, W) or (B, H, W, C)\n",
        "    masks: numpy array (B, 1, H, W) or (B, H, W)\n",
        "    preds: optional, same shape as masks\n",
        "    mean, std: per-channel normalization (arrays of length C)\n",
        "    bands: which bands to display as RGB (default: 0,1,2)\n",
        "    \"\"\"\n",
        "    batch_size = images.shape[0]\n",
        "    for i in range(batch_size):\n",
        "        img = images[i]\n",
        "        if mean is not None and std is not None:\n",
        "            img_vis = unnormalize(img, mean, std)\n",
        "        else:\n",
        "            img_vis = img.copy()\n",
        "            if img_vis.shape[0] in [3,4]:\n",
        "                img_vis = img_vis.transpose(1,2,0)\n",
        "        img_vis = img_vis[..., list(bands)]\n",
        "\n",
        "        gt_mask = masks[i]\n",
        "        if gt_mask.ndim == 3:\n",
        "            gt_mask = gt_mask[0]\n",
        "\n",
        "        plt.figure(figsize=(14, 4))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(img_vis)\n",
        "        plt.title(\"Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(gt_mask, cmap=\"gray\")\n",
        "        plt.title(\"Ground Truth Mask\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        if preds is not None:\n",
        "            pred_mask = preds[i]\n",
        "            if pred_mask.ndim == 3:\n",
        "                pred_mask = pred_mask[0]\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.imshow(pred_mask, cmap=\"gray\")\n",
        "            plt.title(\"Predicted Mask\")\n",
        "            plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "# 1. Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. Load model\n",
        "model = UNetWithResNet(n_channels=4, n_classes=1, pretrained=False)\n",
        "model.load_state_dict(torch.load(\"cannabis_segmentation_model.pth\", map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 3. Get a batch from your validation loader\n",
        "batch = next(iter(val_loader))\n",
        "images = batch[\"image\"].to(device)\n",
        "masks = batch[\"mask\"].to(device).float()\n",
        "\n",
        "# 4. Run inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(images)\n",
        "    preds = (outputs > 0.5).float()\n",
        "\n",
        "# 5. Visualize using the helper function\n",
        "images_np = images.cpu().numpy()\n",
        "masks_np = masks.cpu().numpy()\n",
        "preds_np = preds.cpu().numpy()\n",
        "\n",
        "mean = np.array([0.485, 0.456, 0.406, 0.5])\n",
        "std = np.array([0.229, 0.224, 0.225, 0.5])\n",
        "\n",
        "show_batch(images_np, masks_np, preds_np, mean=mean, std=std, bands=(0,1,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f55787a",
      "metadata": {
        "id": "5f55787a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}